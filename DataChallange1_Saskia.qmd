---
title: "Data Challange 1"
author: "Saskia Gianola"
date: "October/November 2024"
format: 
  html:
    embed-resources: true
    fig_caption: true
    fig-align: center
    highlight: tango
    number-sections: false
    theme: journal
    toc: true
    toc_depth: 2
    toc_float: true
execute:                      # Set global options for execution of code blocks
  echo: true
  warning: false
  message: false
---
```{r, echo=FALSE}
library(dplyr)
library(ggplot2)
library(lubridate)
library(sf)
library(cluster)
library(dbscan)
library(fpc)
library(concaveman)
library(adehabitatHR)
library(spatstat)
library(spdep)
```

# First part: Spatial Clustering ####
* **Task 1:** Download and import the road accident data from Open Data Zurich (ODZ) using an appropriate file format. If needed, transform to the Swiss projected CH1903+/LV95 coordinate system (EPSG = 2056).

```{r, results='hide'}
accidents <- st_read("Session2/roadtrafficaccidentlocations.gpkg")
boundary <-st_read("Session2/Zurich_city_boundary_2024.gpkg")
districts <-st_read("Session2/Zurich_city_districts_2024.gpkg")
```

* **Task 2:** Report the following numbers in table(s):
  
```{r, echo=FALSE}
accidents_no_geom <- st_drop_geometry(accidents) # drop geometry to make code more efficient

acc_sever <- accidents_no_geom |> 
   count(AccidentSeverityCategory)

acc_type <- accidents_no_geom |> 
  count(AccidentType)

acc_ped <- accidents_no_geom |> 
  count(AccidentInvolvingPedestrian)

acc_bike <- accidents_no_geom |> 
  count(AccidentInvolvingBicycle)

acc_moto <- accidents_no_geom |> 
  count(AccidentInvolvingMotorcycle)

acc_ped_moto <- accidents_no_geom |> 
  count(AccidentInvolvingPedestrian, AccidentInvolvingMotorcycle)

acc_ped_bike <- accidents_no_geom |> 
  count(AccidentInvolvingPedestrian, AccidentInvolvingBicycle)

acc_moto_bike <- accidents_no_geom |> 
  count(AccidentInvolvingMotorcycle, AccidentInvolvingBicycle)

acc_ped_bike_moto <- accidents_no_geom |> 
  count(AccidentInvolvingPedestrian, AccidentInvolvingBicycle, AccidentInvolvingMotorcycle)
  
```
  a. Number of accidents by accident severity category
  
```{r}
acc_sever
```
  
  b. Number of accidents by accident type
  
```{r}
acc_type
```

  
  c. Number of accidents involving pedestrians, bicycles, and motorcycles, respectively. And combinations thereof (pedestrian AND bicycle, pedestrian AND motorcycle etc.). Are there any accidents involving all three modes (pedestrian, bicycle, motorcycle)?
  
```{r}
acc_ped
acc_bike
acc_moto
acc_ped_bike
acc_ped_moto
acc_moto_bike
acc_ped_bike_moto
```

  
* **Task 3:** Generate a plot showing the temporal evolution of the number of accidents from 2011 to 2023. Label each year with the corresponding number of accidents. Choose a plot type that is suitable for this type of temporal data. Bonus: Show also the data for the bicycle accidents (cf. Task 4) in the same plot.

```{r, echo=FALSE}
numyearbi <- accidents |> group_by(AccidentYear, AccidentInvolvingBicycle) |>
  summarise(n=n())
numyear <- numyearbi |> group_by(AccidentYear)  |> 
  summarise(n=n())
  

ggplot(numyearbi, aes(x=AccidentYear, y=n,fill=AccidentInvolvingBicycle, label=n)) +
  geom_bar(position="stack", stat="identity")+
  geom_text(size = 3, position = position_stack(vjust = 0.5)) +
  ylab("Number of accidents") +
  xlab("Year") +
  ggtitle("Total Accidents")+
  xlab("Year")+
  ylab("Number of accidents")+
  theme_bw()+
  guides(fill=guide_legend(title="Accident involves bicycles"))

```

* **Task 4:** Select only those accidents that involved a bicycle. **From now on, and for the remainder of DC1, we will restrict our analysis to the accidents involving bicycles.** With this subset, produce a map showing the bicycle accident data colored by accident severity category. Use a basemap such as OpenStreetMap and/or the boundary data available on OLAT, so the accidents can be visually and spatially referenced.

```{r, echo=FALSE}
bike_accidents <- accidents |> 
  filter(AccidentInvolvingBicycle == "true")

#Version normal
ggplot() +
  geom_sf(data = bike_accidents, mapping = aes(color = AccidentSeverityCategory)) +
  geom_sf(data = districts, color = "black", fill = NA) +
  coord_sf(datum = NA)+
  theme(legend.position = "bottom", panel.grid.major = element_line(colour = "transparent")) +
  theme_minimal()+
  scale_color_manual(values = c("red",  "gold","orange2", "blue"),
                               name = "Accident Severity",
                               breaks = c("as1", "as2", "as3", "as4"),
                               labels = c("fatalities", 
                                          "light injuries", 
                                          "severe injuries",
                                          "property damage"))
#Version mit alpha
ggplot() +
  geom_sf(data = bike_accidents, mapping = aes(color =AccidentSeverityCategory)) +
  geom_sf(data = districts, color = "black", fill = NA, ) +
  coord_sf(datum = NA)+
  theme(legend.position = "bottom", panel.grid.major = element_line(colour = "transparent")) +
  theme_minimal()+
  scale_color_manual(values = c(alpha("red",alpha = 1.0),  alpha("gold",alpha = 0.8),alpha("orange2",alpha = 0.8), alpha("blue",alpha = 0.8)),
                               name = "Accident Severity",
                               breaks = c("as1", "as2", "as3", "as4"),
                               labels = c("fatalities", 
                                          "light injuries", 
                                          "severe injuries",
                                          "property damage"))

```



* **Task 5:** Imagine you are given the task of detecting spatial clusters of elevated bicycle accident occurrence (without considering their severity). How would you characterize such "bicycle accident clusters"? Try to define properties that can be used to describe and identify such clusters, and that can be used to choose and parameterize a clustering method suitable for the task. Try to use natural, but precise and concise language in your answer.

I would expect the accidents to cluster on certain parts of the streets. Possible locations are intersections, crossings with trams or pedestrians. Also, i would expect them to cluster on places with low visibility, such as corners. Bikes move in a constraint way, meaning they have to follow the roads. This means clusters will be seperated by buildings or any other obstacles. Therefor, it seems quite difficult to me to find clusters that are reliable. There are a lot of spaces without accidents, simply because bikes can't move there. Also, the density of the accidents will vary, just as the density of the streets varies. 



* **Task 6:** From the bicycle accidents, extract the years 2018 to 2021 and compute clusters for each year separately, using a clustering method you deem appropriate for the task, and choose the control parameters appropriately to capture the types of clusters you had in mind in your definition of Task 5. Justify your choice.

```{r, echo=FALSE}
bike_accidents2018 <- st_coordinates(bike_accidents |> 
  filter(AccidentYear == "2018"))
bike_accidents2019 <- st_coordinates(bike_accidents |> 
  filter(AccidentYear == "2019"))
bike_accidents2020 <- st_coordinates(bike_accidents |> 
  filter(AccidentYear == "2020"))
bike_accidents2021 <- st_coordinates(bike_accidents |> 
  filter(AccidentYear == "2021"))

# accidents from 2018
kNNdistplot(bike_accidents2018, k = 3)
abline(h = 500, col = "red")
db2018 <- dbscan(bike_accidents2018, eps = 500, MinPts = 3)
str(db2018)
plot(bike_accidents2018, pch = 19, cex = 0.5, col = db2018$cluster + 1, asp = 1, main = "Bike accidents 2018")

kNNdistplot(bike_accidents2019, k = 3)
abline(h = 500, col = "red")
db2019 <- dbscan(bike_accidents2019, eps = 500, MinPts = 3)
str(db2019)
plot(bike_accidents2019, pch = 19, cex = 0.5, col = db2019$cluster + 1, asp = 1, main = "Bike accidents 2019")

kNNdistplot(bike_accidents2020, k = 3)
abline(h = 420, col = "red")
db2020 <- dbscan(bike_accidents2020, eps = 500, MinPts = 3)
str(db2020)
plot(bike_accidents2020, pch = 19, cex = 0.5, col = db2020$cluster + 1, asp = 1, main = "Bike accidents 2020")

kNNdistplot(bike_accidents2021, k = 3)
abline(h = 415, col = "red")
db2021 <- dbscan(bike_accidents2021, eps = 500, MinPts = 3)
str(db2021)
plot(bike_accidents2021, pch = 19, cex = 0.5, col = db2021$cluster + 1, asp = 1, main = "Bike accidents 2021")


```


First I performed a DBSCAN cluster analysis. But 500 m seem a lot for the distance. Also, the density varies, which is a weakness of DBSCAN. Therefore, I tried using optics.

```{r, echo=FALSE}
ops_reach2018 <- optics(bike_accidents2018)
ops_reach2018 <- extractDBSCAN(ops_reach2018, eps_cl = 500)
ops_reach2018
plot(ops_reach2018)
plot(bike_accidents2018, cex = 0.4, pch = 19, col = ops_reach2018$cluster + 1, asp = 1)
ops_reach2018 <- extractDBSCAN(ops_reach2018, eps_cl = 250)
ops_reach2018
plot(ops_reach2018)
plot(bike_accidents2018, cex = 0.4, pch = 19, col = ops_reach2018$cluster + 1, asp = 1)
hullplot(bike_accidents2018, ops_reach2018, asp = 1)


ops_reachXi2018 <- extractXi(ops_reach2018, xi = 0.06)
plot(ops_reachXi2018)
hullplot(bike_accidents2018, ops_reachXi2018, asp = 1)

```


* **Task 7**: Discuss your results, including also limitations or problems, and possible other methods that you could have used.

Even with the eye it is hard to identify clusters. Using optics with the same threshold of 500 m, most accidents are classified as one cluster. But if the eps is lowered to 250, more cluster are visible and the clustering makes more sense. With extractXi it's possible to make even better clusters. Nevertheless, generally it can be said that in the centre of Zurich, there are a lot of bike accidents. The accidents happen on roads, which is clearly visible. The density of accidents lowers with rising distance to the centre, just as roads do. The thresholds could be adapted even more, maybe also using a streetmap as background. For further exercises, I will be using the DBSCAN clusters.

# Second part: Polygon Delineation ####
* **Task 8**: Given the clusters that you have extracted in Part 1 of the DC1 assignment:
  a. Define a set of criteria that a method should fulfill that can be used to delineate the given clusters by polygons. Use free text for these definitions, but try to be concise and precise. (Note: These criteria can also be used in the subsequent Discussion to evaluate whether they have been met.)
  
As described above, the accidents happen along roads, maybe even more on crossings. The method should be able to form round clusters for the crossings as well as longer shaped polygones to determine roads with more accidents.  

  b. Choose a polygon delineation method that you deem appropriate in light of the above criteria. Justify your choice.
  
For this task, I chose the concave hull as I want the polygone to include different froms of clusters, but I don't want it to generate holes within the clusters. Also, the clusters should not overlap. 


* **Task 9**: From the years 2018 to 2021 for which you computed clusters in Task 6, choose at least two years and apply your polygon delineation method of choice to each of these two years separately. Compute the Jaccard Index (aka Intersection over Union) for pair(s) of selected years and present and discuss the results.

```{r , echo = FALSE}
# add cluster info to data
bike_acc2018_clus <- st_as_sf(as.data.frame(cbind(bike_accidents2018, db2018$cluster)), 
                              coords = c("X", "Y"), crs = 2056)
bike_acc2019_clus <- st_as_sf(as.data.frame(cbind(bike_accidents2019, db2019$cluster)), 
                              coords = c("X", "Y"), crs = 2056)

summary(bike_acc2018_clus) # for 2018 data, there are 8 clusters
summary(bike_acc2019_clus) # for 2019 data, there are 10 clusters

# separate data into single clusters

bike_acc2018_c1 <- bike_acc2018_clus |> 
  filter(V4 == 1)
bike_acc2018_c2 <- bike_acc2018_clus |> 
  filter(V4 == 2)
bike_acc2018_c3 <- bike_acc2018_clus |> 
  filter(V4 == 3)
bike_acc2018_c4 <- bike_acc2018_clus |> 
  filter(V4 == 4)
bike_acc2018_c5 <- bike_acc2018_clus |> 
  filter(V4 == 5)
bike_acc2018_c6 <- bike_acc2018_clus |> 
  filter(V4 == 6)
bike_acc2018_c7 <- bike_acc2018_clus |> 
  filter(V4 == 7)
bike_acc2018_c8 <- bike_acc2018_clus |> 
  filter(V4 == 8)

bike_acc2019_c1 <- bike_acc2019_clus |> 
  filter(V4 == 1)
bike_acc2019_c2 <- bike_acc2019_clus |> 
  filter(V4 == 2)
bike_acc2019_c3 <- bike_acc2019_clus |> 
  filter(V4 == 3)
bike_acc2019_c4 <- bike_acc2019_clus |> 
  filter(V4 == 4)
bike_acc2019_c5 <- bike_acc2019_clus |> 
  filter(V4 == 5)
bike_acc2019_c6 <- bike_acc2019_clus |> 
  filter(V4 == 6)
bike_acc2019_c7 <- bike_acc2019_clus |> 
  filter(V4 == 7)
bike_acc2019_c8 <- bike_acc2019_clus |> 
  filter(V4 == 8)
bike_acc2019_c9 <- bike_acc2019_clus |> 
  filter(V4 == 9)
bike_acc2019_c10 <- bike_acc2019_clus |> 
  filter(V4 == 10)

# compare thresholds
cave_acc2018_2 <- concaveman(bike_acc2018_c1, concavity = 2, length_threshold = 0)
cave_acc2018_1 <- concaveman(bike_acc2018_c1, concavity = 1, length_threshold = 0)
cave_acc2018_05 <- concaveman(bike_acc2018_c1, concavity = 0.5, length_threshold = 0)


ggplot() +
  geom_sf(data = districts, color = "black", fill = NA) +
  geom_sf(data=bike_acc2018_c1, size = 0.5) +
  geom_sf(data = cave_acc2018_2, color = "cyan", fill = alpha("cyan", alpha=0.2)) +
  geom_sf(data = cave_acc2018_1, color = "magenta", fill = alpha("magenta", alpha=0.2)) +
  geom_sf(data = cave_acc2018_05, color = "yellow", fill = alpha("yellow", alpha=0.2)) +
  coord_sf(datum = NA)+
  ggtitle("find best concavity value")+
  theme_minimal()

# 1 seems best value for concavity
bike_acc2018_c1_con <- concaveman(bike_acc2018_c1, concavity = 1, length_threshold = 0)
bike_acc2018_c2_con <- concaveman(bike_acc2018_c2, concavity = 1, length_threshold = 0)
bike_acc2018_c3_con <- concaveman(bike_acc2018_c3, concavity = 1, length_threshold = 0)
bike_acc2018_c4_con <- concaveman(bike_acc2018_c4, concavity = 1, length_threshold = 0)
bike_acc2018_c5_con <- concaveman(bike_acc2018_c5, concavity = 1, length_threshold = 0)
bike_acc2018_c6_con <- concaveman(bike_acc2018_c6, concavity = 1, length_threshold = 0)
bike_acc2018_c7_con <- concaveman(bike_acc2018_c7, concavity = 1, length_threshold = 0)
bike_acc2018_c8_con <- concaveman(bike_acc2018_c8, concavity = 1, length_threshold = 0)

bike_acc2019_c1_con <- concaveman(bike_acc2019_c1, concavity = 1, length_threshold = 0)
bike_acc2019_c2_con <- concaveman(bike_acc2019_c2, concavity = 1, length_threshold = 0)
bike_acc2019_c3_con <- concaveman(bike_acc2019_c3, concavity = 1, length_threshold = 0)
bike_acc2019_c4_con <- concaveman(bike_acc2019_c4, concavity = 1, length_threshold = 0)
bike_acc2019_c5_con <- concaveman(bike_acc2019_c5, concavity = 1, length_threshold = 0)
bike_acc2019_c6_con <- concaveman(bike_acc2019_c6, concavity = 1, length_threshold = 0)
bike_acc2019_c7_con <- concaveman(bike_acc2019_c7, concavity = 1, length_threshold = 0)
bike_acc2019_c8_con <- concaveman(bike_acc2019_c8, concavity = 1, length_threshold = 0)
bike_acc2019_c9_con <- concaveman(bike_acc2019_c9, concavity = 1, length_threshold = 0)
bike_acc2019_c10_con <- concaveman(bike_acc2019_c10, concavity = 1, length_threshold = 0)

ggplot() +
  geom_sf(data = districts, color = "black", fill = NA) +
  geom_sf(data = bike_acc2018_c1, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2018_c1_con, color = "pink", fill = alpha("pink", alpha=0.3)) + 
  geom_sf(data = bike_acc2018_c2, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2018_c2_con, color = "purple", fill = alpha("purple", alpha=0.3)) + 
  geom_sf(data = bike_acc2018_c3, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2018_c3_con, color = "seagreen", fill = alpha("seagreen", alpha=0.3)) + 
  geom_sf(data = bike_acc2018_c4, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2018_c4_con, color = "chartreuse", fill = alpha("chartreuse", alpha=0.3)) + 
  geom_sf(data = bike_acc2018_c5, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2018_c5_con, color = "blue", fill = alpha("blue", alpha=0.3)) + 
  geom_sf(data = bike_acc2018_c6, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2018_c6_con, color = "lightblue", fill = alpha("lightblue", alpha=0.3)) + 
  geom_sf(data = bike_acc2018_c7, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2018_c7_con, color = "orange", fill = alpha("orange", alpha=0.3)) + 
  geom_sf(data = bike_acc2018_c8, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2018_c8_con, color = "red", fill = alpha("red", alpha=0.3)) +
  theme_minimal() +
  coord_sf(datum = NA)+
  ggtitle("Clusters of bike accidents 2018")

ggplot() +
  geom_sf(data = districts, color = "black", fill = NA) +
  geom_sf(data = bike_acc2019_c1, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2019_c1_con, color = "pink", fill = alpha("pink", alpha=0.3)) + 
  geom_sf(data = bike_acc2019_c2, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2019_c2_con, color = "purple", fill = alpha("purple", alpha=0.3)) + 
  geom_sf(data = bike_acc2019_c3, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2019_c3_con, color = "seagreen",  fill = alpha("seagreen", alpha=0.3)) + 
  geom_sf(data = bike_acc2019_c4, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2019_c4_con, color = "chartreuse",fill = alpha("chartreuse", alpha=0.3)) + 
  geom_sf(data = bike_acc2019_c5, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2019_c5_con, color = "blue", fill = alpha("blue", alpha=0.3)) + 
  geom_sf(data = bike_acc2019_c6, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2019_c6_con, color = "lightblue",  fill = alpha("lightblue", alpha=0.3)) + 
  geom_sf(data = bike_acc2019_c7, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2019_c7_con, color = "orange", fill = alpha("orange", alpha=0.3))  + 
  geom_sf(data = bike_acc2019_c8, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2019_c8_con, color = "red",  fill = alpha("red", alpha=0.3)) +
  geom_sf(data = bike_acc2019_c9, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2019_c9_con, color = "gold",  fill = alpha("gold", alpha=0.3)) + 
  geom_sf(data = bike_acc2019_c10, color = "black", size = 0.5) +
  geom_sf(data = bike_acc2019_c10_con, color = "magenta",  fill = alpha("magenta", alpha=0.3))+
  theme_minimal() +
  coord_sf(datum = NA)+
  ggtitle("Clusters of bike accidents 2019")

# Compute the Jaccard Index
## unify single polygones from 2018 and 2019
poly2018 <- st_union(bike_acc2018_c1_con, 
                     bike_acc2018_c2_con,
                     bike_acc2018_c3_con,
                     bike_acc2018_c4_con,
                     bike_acc2018_c5_con,
                     bike_acc2018_c6_con,
                     bike_acc2018_c7_con,
                     bike_acc2018_c8_con)
poly2019 <- st_union(bike_acc2019_c1_con, 
                     bike_acc2019_c2_con,
                     bike_acc2019_c3_con,
                     bike_acc2019_c4_con,
                     bike_acc2019_c5_con,
                     bike_acc2019_c6_con,
                     bike_acc2019_c7_con,
                     bike_acc2019_c8_con,
                     bike_acc2019_c9_con,
                     bike_acc2019_c10_con)

## compute difference
poly2018_2019_diff <- st_difference(poly2018, poly2019)
poly2019_2018_diff <- st_difference(poly2019, poly2018)
poly_18_19diff <- st_union(poly2018_2019_diff, poly2019_2018_diff)

plot(st_coordinates(poly2018), xlab = "Easting [m]", ylab = "Northing [m]", 
     type = "n", asp = 1)
plot(poly2018, col =alpha("cyan", 0.3), border = NA, add = TRUE)
plot(poly2019, col =alpha("magenta", 0.3) , border = NA, add = TRUE)
plot(poly_18_19diff, border = "purple", add = TRUE)

cat("Area of 2018:", sf::st_area(poly2018), "m^2 \n",
    "Area of 2019: ", sf::st_area(poly2019), "m^2 \n",
    "Difference Area:    ", sf::st_area(poly_18_19diff), "m^2 \n")

```
* **Task 10**: Overall, what did you find with the above steps? What do these steps tell you about the situation of bicycle accidents in Zurich? How useful are the methods used so far in analysing the given data? Any other points of note?

The last plot with the difference in the area shows that over the two years, the biggest overlap is in the centre of the city. The accidents in the surrounding area varies from 2018 to 2019. For better understanding of the accidents, such an analysis should be done over all available data. If the parameters are refined, the results will probalbly tell us something about where, over many years, most bike accidents happen. To have a closer look at the centre, where most accidents seem to happen, this are should be investigated more detailed, for example by excluding accidents outside certain districts. Nevetheless, such analysis can help finding hotspots in bike accidents. 


# Third part: Density estimation ####
* **Task 11:** Similarly to the clustering and polygon delineation tasks carried out in Parts 1 and 2 of DC1, respectively, start off by defining criteria for using KDE to detect areas/hotspots of elevated bicycle accident density, and explain your reasoning.

KOMMENTAR


* **Task 12:** Choose any **two years** from the years 2018 to 2021 (justify your choice of years) and compute the KDE surfaces for each of these two separately and visualize your results. You are free to choose the KDE implementation (i.e., R package and function(s)) as well as the parameters (bandwidth selection method, etc.), but you should document your choices and discuss, in the subsequent Task 14, your results in light of your choices.

```{r}
# set parameters
ext_val <- 0.3    # 0.3
grid_val <- 300   # 300

# get sf data for bike accidents in 2018 and 2019
bike_accidents_sf <- bike_accidents|> 
  filter(bike_accidents$AccidentInvolvingBicycle == "true")

bike_accidents_2018_sf <- bike_accidents_sf |> 
  filter(bike_accidents_sf$AccidentYear == 2018)
bike_accidents_2019_sf <- bike_accidents_sf |> 
  filter(bike_accidents_sf$AccidentYear == 2019)

# drop everything except geometry
bike_accidents_2018_og <- st_zm(bike_accidents_2018_sf[,22])
bike_accidents_2019_og <- st_zm(bike_accidents_2019_sf[,22])

# convert to SpatialPointsDataFrame
bike_acc2018_point <- as(bike_accidents_2018_og, "Spatial")
bike_acc2019_point <- as(bike_accidents_2019_og, "Spatial")

# First, use the reference bandwidth method for bandwith selection (h = "href")

# Compute UD 2018
ud_18 <- adehabitatHR::kernelUD(bike_acc2018_point, grid = grid_val, extent = ext_val, h = "href")
hr95_18 <- adehabitatHR::getverticeshr(ud_18, percent = 95)   # retrieve home range (95th volume percentile)
hr50_18 <- adehabitatHR::getverticeshr(ud_18, percent = 50)   # retrieve core area (50th volume percentile)

# Compute UD 2019
ud_19 <- adehabitatHR::kernelUD(bike_acc2019_point, grid = grid_val, extent = ext_val, h = "href")
hr95_19 <- adehabitatHR::getverticeshr(ud_19, percent = 95)   # retrieve home range (95th volume percentile)
hr50_19 <- adehabitatHR::getverticeshr(ud_19, percent = 50)   # retrieve core area (50th volume percentile)

# plot KDE for 2018 data
graphics::image(ud_18, xlab = "x [m]", ylab = "y [m]",
                col = hcl.colors(200, palette = "heat 2", rev = TRUE))
xmin <- min(ud_18$ZH@coords[,1])
xmax <- max(ud_18$ZH@coords[,1])
ymin <- min(ud_18$ZH@coords[,2])
ymax <- max(ud_18$ZH@coords[,2])
plot(hr50_18, lty = 4, lwd = 3, border = "black", add = TRUE, axes = FALSE)
plot(hr95_18, lty = 1, lwd = 2, border = "blue", add = TRUE, axes = FALSE)
axis(1)
axis(2, pos = xmin - 100)
text(xmin - 150, ymin + (ymax - ymin) / 2, "y [m]", 
     adj = c(NA, -4), srt = 90)
title("KDE with bandwidth selection method HREF", line = -0.3)
legend("topright", c("HR 50%", "HR 95%"), 
       col = c("black", "blue"), lwd = c(3, 2), lty = c(4, 1), 
       inset = c(0.19, 0.06), cex = 0.75)
cat("Size of home range with HREF (95 %): ", hr95_18$area, sep = "", "\n")
cat("Size of core area with HREF (50 %): ", hr50_18$area, sep = "", "\n")

# plot KDE for 2019 data
graphics::image(ud_19, xlab = "x [m]", ylab = "y [m]",
                col = hcl.colors(200, palette = "heat 2", rev = TRUE))
xmin <- min(ud_19$ZH@coords[,1])
xmax <- max(ud_19$ZH@coords[,1])
ymin <- min(ud_19$ZH@coords[,2])
ymax <- max(ud_19$ZH@coords[,2])
plot(hr50_19, lty = 4, lwd = 3, border = "black", add = TRUE, axes = FALSE)
plot(hr95_19, lty = 1, lwd = 2, border = "blue", add = TRUE, axes = FALSE)
axis(1)
axis(2, pos = xmin - 100)
text(xmin - 150, ymin + (ymax - ymin) / 2, "y [m]", 
     adj = c(NA, -4), srt = 90)
title("KDE with bandwidth selection method HREF", line = -0.3)
legend("topright", c("HR 50%", "HR 95%"), 
       col = c("black", "blue"), lwd = c(3, 2), lty = c(4, 1), 
       inset = c(0.19, 0.06), cex = 0.75)
cat("Size of home range with HREF (95 %): ", hr95_19$area, sep = "", "\n")
cat("Size of core area with HREF (50 %): ", hr50_19$area, sep = "", "\n")

# Now use the reference least-squares cross-validation nethod (h = "LSCV")
# Positioning of legend box is optimized for knitr HTML output
ud <- adehabitatHR::kernelUD(bike_acc2018_point, grid = grid_val, extent = ext_val, h = "LSCV")
hr95 <- adehabitatHR::getverticeshr(ud, percent = 95)   # retrieve home range (95th volume percentile)
hr50 <- adehabitatHR::getverticeshr(ud, percent = 50)   # retrieve core area (50th volume percentile)

graphics::image(ud, xlab = "x [m]", ylab = "y [m]", 
                col = hcl.colors(200, palette = "heat 2", rev = TRUE))
xmin <- min(ud$ZH@coords[,1])
xmax <- max(ud$ZH@coords[,1])
ymin <- min(ud$ZH@coords[,2])
ymax <- max(ud$ZH@coords[,2])
plot(hr50, lty = 4, lwd = 3, border = "black", add = TRUE, axes = FALSE)
plot(hr95, lty = 1, lwd = 2, border = "blue", add = TRUE, axes = FALSE)
axis(1)
axis(2, pos = xmin - 100)
text(xmin - 150, ymin + (ymax - ymin) / 2, "y [m]", 
     adj = c(NA, -4), srt = 90)
title("KDE with bandwidth selection method LSCV", line = -0.3)
legend("topright", c("HR 50%", "HR 95%"), col = c("black", "blue"), 
       lwd = c(3, 2), lty = c(4, 1), inset = c(0.19, 0.06), cex = 0.75)
cat("Size of home range with LSCV (95 %): ", hr95$area, sep = "", "\n")
cat("Size of core area with LSCV (50 %): ", hr50$area, sep = "", "\n")
```

* **Task 13:** Compute the "volume of intersection" ("VI") between the KDE surfaces (utilization distributions) of the two years. Hint: There are different ways to do that, but the `adehabitatHR` package has functionality for that. How do the results correspond to those of Task 9 (Jaccard Index or IoU)?

```{r}

```

* **Task 14:** Discuss your results for this part of DC1 (density estimation). What did you find? Compare the results of this part with the clusters/polygons of Parts 1 and 2 (see note below): What are the commonalities? What are the differences? Which method(s) perform more adequately than others for the given problem and data? Which method(s) would you recommend, and which ones not? Why? (You are free to add more points to the discussion.)

KOMMENTAR


# Fourth part: Second Order Properties ####
* **Task 15:** Choose one or more distance measure functions (justify your choice) and compute
it/them for * a. all bicycle accidents (2011 - 2021) 
I chose the L-function as it's based on the K-function but linear and thus easier to intrepret if there is correlation. 

::: {.panel-tabset .nav-pills}

## L-Function

```{r, echo=FALSE}
# Create a ppp object 
bike_accidents_coords <- sf::st_coordinates(bike_accidents_sf)
win <- as.owin(st_bbox(bike_accidents_sf))
bike_accidents_ppp <- spatstat.geom::ppp(x = bike_accidents_coords[,1], y =  bike_accidents_coords[,2],
                                         window = win, unitname = c("meter", "meters"))

# Generate L-function with envelope() function.
bike_accidents_enve <- spatstat.explore::envelope(bike_accidents_ppp, fun = Lest, nsim = 100, 
                                         correction = "Ripley", verbose = FALSE)
# Plot 
plot(bike_accidents_enve, main = "L-function of bike acccident points, with envelope")

```
## MAD & DCLF Test
```{r, echo=FALSE}
# We also apply the MAD test
spatstat.explore::mad.test(bike_accidents_ppp, fun = Lest, nsim = 100, verbose = FALSE)
spatstat.explore::dclf.test(bike_accidents_ppp, fun = Lest, nsim = 100, verbose = TRUE)

```
:::

* b. bicycle accidents for one of the years selected in Task 6 

::: {.panel-tabset .nav-pills}
## L-Function 2018 data
```{r, echo=FALSE}
# Create a ppp object 
bike_accidents2018_coords <- sf::st_coordinates(bike_accidents_2018_sf)
win18 <- as.owin(st_bbox(bike_accidents_2018_sf))
bike_accidents2018_ppp <- spatstat.geom::ppp(x = bike_accidents2018_coords[,1], y =  bike_accidents2018_coords[,2],
                                         window = win18, unitname = c("meter", "meters"))

# Generate L-function
bike_accidents2018_enve <- spatstat.explore::envelope(bike_accidents2018_ppp, fun = Lest, nsim = 100, 
                                         correction = "Ripley", verbose = FALSE)
# Plot 
plot(bike_accidents2018_enve, main = "L-function of 2018 bike acccident points, with envelope")

```

## MAD & DCLF Test 2018 data
```{r, echo=FALSE}
# We also apply the MAD test
spatstat.explore::mad.test(bike_accidents2018_ppp, fun = Lest, nsim = 100, verbose = FALSE)
spatstat.explore::dclf.test(bike_accidents2018_ppp, fun = Lest, nsim = 100, verbose = TRUE)

```

## L-Function 2019 data
```{r, echo=FALSE}
# Create a ppp object 
bike_accidents2019_coords <- sf::st_coordinates(bike_accidents_2019_sf)
win19 <- as.owin(st_bbox(bike_accidents_2019_sf))
bike_accidents2019_ppp <- spatstat.geom::ppp(x = bike_accidents2019_coords[,1], y =  bike_accidents2019_coords[,2],
                                         window = win19, unitname = c("meter", "meters"))

# Generate L-function
bike_accidents2019_enve <- spatstat.explore::envelope(bike_accidents2019_ppp, fun = Lest, nsim = 100, 
                                         correction = "Ripley", verbose = FALSE)
# Plot 
plot(bike_accidents2019_enve, main = "L-function of 2019 bike acccident points, with envelope")
```

## MAD & DCLF Test 2019 data
```{r, echo=FALSE}
# We also apply the MAD test
spatstat.explore::mad.test(bike_accidents2019_ppp, fun = Lest, nsim = 100, verbose = FALSE)
spatstat.explore::dclf.test(bike_accidents2019_ppp, fun = Lest, nsim = 100, verbose = TRUE)
```

::: 

* c. bicycle accidents for only the cluster points of the year selected in (b)
Restrict your analysis to the “inner city” and use the same window for all point sets. It’s up to you to define the extent of the “inner city” and explain/justify what that means in terms of this data challenge.

```{r}


```



* **Task 16:** Now choose the following two pairs of years, 2018 & 2019, as well as 2018 & 2021, and compute the cross-X function, where “X” stands for the function(s) you used in Task 15. Use the AccidentYear as the marks to produce a marked point pattern.

::: {.panel-tabset .nav-pills}

## marked point pattern
```{r, echo=FALSE}
# reate a marked ppp object
marks_bike_accidents_ppp <- spatstat.geom::setmarks(bike_accidents_ppp, factor(bike_accidents_sf$AccidentYear))

bike_accidents181921 <- bike_accidents_sf |> 
  filter(bike_accidents_sf$AccidentYear %in% c("2018", "2019", "2021"))

# Summary will show frequency table per Year
summary(bike_accidents181921)

ggplot() +
  geom_sf(data = bike_accidents181921, aes(colour = AccidentYear))  +
  coord_sf(datum = 2056) +
  ggtitle("Marked point pattern of bike accidents 2018, 2019 and 2021") +
  xlab("Easting [m]") + 
  ylab("Northing [m]") +
  theme(plot.title = element_text(face="bold"))
```

## Cross-L function 2018 and 2019
```{r, echo=FALSE}
acc_2018_2019_clf_env <- spatstat.explore::envelope(marks_bike_accidents_ppp, fun = Lcross, 
                                                    i = "2018", j = "2019", nsim = 100, 
                                                    correction = "Ripley", verbose = FALSE)

# Plot it
plot(acc_2018_2019_clf_env, main = "Cross-L-function of bike accidents 2018 vs. 2019")
```

## Cross-L function 2018 and 2021
```{r, echo=FALSE}
#create ppp for 2021 data
bike_accidents_2021_sf <- bike_accidents_sf |> 
  filter(bike_accidents_sf$AccidentYear == 2021)

# drop everything except geometry
bike_accidents_2021_sf <- st_zm(bike_accidents_2021_sf[,22])

bike_accidents2021_coords <- sf::st_coordinates(bike_accidents_2021_sf)
win21 <- as.owin(st_bbox(bike_accidents_2021_sf))
bike_accidents2021_ppp <- spatstat.geom::ppp(x = bike_accidents2021_coords[,1], y =  bike_accidents2021_coords[,2],
                                         window = win21, unitname = c("meter", "meters"))


acc_2018_2021_clf_env <- spatstat.explore::envelope(marks_bike_accidents_ppp, fun = Lcross, 
                                                    i = "2018", j = "2021", nsim = 100, 
                                                    correction = "Ripley", verbose = FALSE)

# Plot it
plot(acc_2018_2021_clf_env, main = "Cross-L-function of bike accidents 2018 vs. 2021")
```
:::

* **Task 17:** Discuss your results. Similarities and differences between the various point sets? Noteworthy spatial and/or temporal patterns? Is any difference observable in the patterns of the cross-X function in the transition to the Covid-19 pandemic? etc. etc. Note: Consider in your interpretation that the distance scale may change between years.

KOMMENTAR


* **Task 18:** From the bicycle accidents data, choose at least two relevant variables that make sense being compared, e.g. two different accident types, severity levels, times of day, years etc. For these selected accidents, compute the counts within the ‘statistical zones’ of Zurich and use these counts to compute the Getis-Ord G*-statistic for each of your counts layers. Visualize your results appropriately.

```{r}
# read data of statistical zones
zh_stat_zone <- sf::st_read("zh_stat_zones.gpkg")

# Find nearest neighbors within 500m
# You should play with nn.dis and see what it does to connectivity.
nn_dis <- 2000

# create list of euclidean neighbors with min. distance = 0 and max. distance = nn_dis 
bike_acc_nb <- spdep::dnearneigh(bike_accidents_sf, 0, nn_dis)

# convert into list with spatial weights, B = binary weight
bike_acc_lw <- spdep::nb2listw(spdep::include.self(bike_acc_nb), style = "B")

# Create the line links from the nb object, requesting an sf object to be returned.
# CRS needs to be set as it was carried over to the nb object.
# bike_acc_links <- 
#   spdep::nb2lines(bike_acc_nb, coords = sf::st_geometry(bike_accidents_sf), as_sf = TRUE) |>
#   sf::st_set_crs(2056)
# 
# # Plot the neighbor links using functions from the tmap package
# tmap::tm_shape(zh_stat_zone) + 
#   tmap::tm_borders(col = "gray") +
#   tmap::tm_shape(bike_acc_links) + 
#   tmap::tm_lines(col = "red", lwd = 0.5) +
#   tmap::tm_shape(bike_accidents_sf) +
#   tmap::tm_dots(col = "black", size = 0.1) +
#   tmap::tm_scale_bar(position = c("right", "bottom"), width = 0.2) +
#   tmap::tm_layout(
#     title = paste("Neighbor links within ",
#                   nn_dis / 1000, " km", sep = ""),
#     title.position = c("left", "top"),
#     title.size = 1.1,
#     title.fontface = "bold",
#     frame = FALSE,
#     inner.margins = c(0.05, 0.05, 0.05, 0.05)
#  )
```

```{r}
#| label: g-stat-compute


```


* **Task 19:** Discuss your results. What did you find regarding the hot and cold spots in your accident count layers? How do they compare to each other across layers and across (past) methods? etc. Discuss also the influence of the parameter settings (e.g., neighbor search distance, spatial weight formation) on your results.

KOMMENTAR

# Time spend on DC1 and lab partner ####
* Part 1: 2.5h
* Part 2: 2.5h
* Part 3: 2.5h
* Part 4:
Lab partner: 


