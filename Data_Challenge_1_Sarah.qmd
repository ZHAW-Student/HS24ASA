---
title: "Ex2_Sarah"
author: "Sarah Wirth"
date: "25.09.2024"
format: 
  html:
    fig_caption: true
    fig-align: center
    highlight: tango
    number-sections: false
    theme: flatly
    toc: true
    toc_depth: 2
    toc_float: true

execute:                     
  echo: true
  warning: false
  message: false
---
```{r}
library(dplyr)
library(ggplot2)
library(lubridate)
library(sf)
library(spatstat)
```

# First part: Spatial Clustering ####

* **Task 0:** Create a new R Project and new R Script using a clean template and initialization of the environment. We recommend you use RMarkdown with HTML as target output.

* **Task 1:** Download and import the road accident data from Open Data Zurich (ODZ) using an appropriate file format. If needed, transform to the Swiss projected CH1903+/LV95 coordinate system (EPSG = 2056).
```{r, echo = FALSE, message = FALSE}
accidents <- sf::st_read("Session_2/roadtrafficaccidentlocations.gpkg")

boundary <-sf::st_read("Session_2/Zurich_city_boundary_2024.gpkg")

districts <-sf::st_read("Session_2/Zurich_city_districts_2024.gpkg")

```

* **Task 2:** Report the following numbers in table(s):
  a. Number of accidents by accident severity category
  b. Number of accidents by accident type
  c. Number of accidents involving pedestrians, bicycles, and motorcycles, respectively. And combinations thereof (pedestrian AND bicycle, pedestrian AND motorcycle etc.). Are there any accidents involving all three modes (pedestrian, bicycle, motorcycle)? no
  
  
  
```{r, echo = FALSE, message = FALSE}
numsev<-accidents |> 
  count(AccidentSeverityCategory_en)

numtype <- accidents |> 
  count(AccidentType_en)

numped <- accidents |> 
  count(AccidentInvolvingPedestrian)

numbi <- accidents |> 
  count(AccidentInvolvingBicycle)

nummot <- accidents |> 
  count(AccidentInvolvingMotorcycle)

numpedbimot <- accidents |> 
  group_by(AccidentInvolvingBicycle, AccidentInvolvingMotorcycle, AccidentInvolvingPedestrian) |> 
  summarise(n=n())

```



* **Task 3:** Generate a plot showing the temporal evolution of the number of accidents from 2011 to 2023. Label each year with the corresponding number of accidents. Choose a plot type that is suitable for this type of temporal data. Bonus: Show also the data for the bicycle accidents (cf. Task 4) in the same plot.
```{r, echo = FALSE, message = FALSE}
numyearbi <- accidents |> group_by(AccidentYear, AccidentInvolvingBicycle) |> 
  summarise(n=n())

numyear <- numyearbi |> group_by(AccidentYear)  |> 
  summarise(n=n())
  
ggplot(numyearbi, aes(x=AccidentYear, y=n,fill=AccidentInvolvingBicycle)) +
  geom_bar(position="stack", stat="identity")+
  xlab("Year")+
  ylab("")+
  guides(fill=guide_legend(title="Bicycle"))

```

* **Task 4:** Select only those accidents that involved a bicycle. **From now on, and for the remainder of DC1, we will restrict our analysis to the accidents involving bicycles.** With this subset, produce a map showing the bicycle accident data colored by accident severity category. Use a basemap such as OpenStreetMap and/or the boundary data available on OLAT, so the accidents can be visually and spatially referenced.
```{r, echo = FALSE, message = FALSE}
onlybikes <-accidents |>  
  filter(AccidentInvolvingBicycle== "true")#7425 obs

cols<- c("Accident with fatalities" ="red", "Accident with light injuries"="yellow","Accident with property damage"= "blue", "Accident with severe injuries"="orange")


ggplot()+
  geom_sf(data=boundary, aes(color="white"))+
  geom_sf(data=onlybikes, aes(color=factor(AccidentSeverityCategory_en)))+
  scale_colour_manual(values = cols)+
  theme(legend.position="none")

```

* **Task 5:** Imagine you are given the task of detecting spatial clusters of elevated bicycle accident occurrence (without considering their severity). How would you characterize such "bicycle accident clusters"? Try to define properties that can be used to describe and identify such clusters, and that can be used to choose and parameterize a clustering method suitable for the task. Try to use natural, but precise and concise language in your answer.


The accidents occur mostly along road networks as bicyles are bound to them except for recreational usage, which probably leads to some mountainbike- associated accidents in the forests. Except for those accidents, I expect clusters in areas with high traffic and a mix of different types traffic participants like at the Central where pedestrians, trams, and road traffic mix. Further I expect visibility and the complexity of traffic routing to lead to clustering at intersections. I also expect several reports from accidents with light injuries and or property damage not to be included in this data based on neighbors dealing with smaller accidents on their own. The type of landuse also will lead to clustering as different landuses will lead to different traffic. As an example I do expect a lower amount of accidents with a lower density in residential areas than in the area of  railroad stations. Based on those assumptions I expect clusters being mostly generated around the different city centres and they will probably be separated by more rural areas.

As a clustering method I would use OPTICS rather than DBSCAN, to adapt to the different densities of accidents between the city centres and residential areas like Schwamendingen.


* **Task 6:** From the bicycle accidents, extract the years 2018 to 2021 and compute clusters for each year separately, using a clustering method you deem appropriate for the task, and choose the control parameters appropriately to capture the types of clusters you had in mind in your definition of Task 5. Justify your choice.
```{r, echo = FALSE, message = FALSE}
onlybikes18 <- sf::st_coordinates(onlybikes |> 
  filter (AccidentYear ==2018))

onlybikes19 <-  sf::st_coordinates(onlybikes |> 
  filter (AccidentYear ==2019))

onlybikes20 <- sf::st_coordinates( onlybikes |> 
  filter (AccidentYear ==2020))

onlybikes21 <- sf::st_coordinates(onlybikes |> 
  filter (AccidentYear ==2021))


library("cluster")
library("dbscan")
library("fpc")
library("seriation")
library("mlbench")
library("spatgraphs")

dbscan::kNNdistplot(onlybikes18, k = 3)
graphics::abline(h = 500, col = "red")

dbscan::kNNdistplot(onlybikes19, k = 3)
graphics::abline(h = 500, col = "red")

dbscan::kNNdistplot(onlybikes20, k = 3)
graphics::abline(h = 500, col = "red")

dbscan::kNNdistplot(onlybikes21, k = 3)
graphics::abline(h = 500, col = "red")

#500 seems to be ok overall


db18 <- dbscan::dbscan(onlybikes18, eps = 500, minPts = 3)
db18

db19 <- dbscan::dbscan(onlybikes19, eps = 500, minPts = 3)
db19

db20 <- dbscan::dbscan(onlybikes20, eps = 500, minPts = 3)
db20

db21 <- dbscan::dbscan(onlybikes21, eps = 500, minPts = 3)
db21

```

```{r, echo = FALSE, message = FALSE}
plot(onlybikes18, pch = 19, cex = 0.5, col = db18$cluster + 1, asp = 1)
title(main="2018")

plot(onlybikes19, pch = 19, cex = 0.5, col = db19$cluster + 1, asp = 1)
title(main="2019")

plot(onlybikes20, pch = 19, cex = 0.5, col = db20$cluster + 1, asp = 1)
title(main="2020")

plot(onlybikes21, pch = 19, cex = 0.5, col = db21$cluster + 1, asp = 1)
title(main="2021")

```


* **Task 7**: Discuss your results, including also limitations or problems, and possible other methods that you could have used.


Overall its difficult to separate the clusters within the city centre and the accidents in rural areas tend not to be included into clusters. 



#########################Mehr

# Second part: Polygon Delineation ####
* **Task 8**: Given the clusters that you have extracted in Part 1 of the DC1 assignment:
  a. Define a set of criteria that a method should fulfill that can be used to delineate the given clusters by polygons. Use free text for these definitions, but try to be concise and precise. (Note: These criteria can also be used in the subsequent Discussion to evaluate whether they have been met.)
  
The method should be capable to account for concave indents in the clusters and the polygons should not overlap with eachother (within a year).
  
  b. Choose a polygon delineation method that you deem appropriate in light of the above
criteria. Justify your choice.

I will use Concave hull as a delineation method, as it does account for the concave shape, but will not make holes in the clusters and can be adapted to minimize overlapping between clusters.


* **Task 9**: From the years 2018 to 2021 for which you computed clusters in Task 6, choose at least two years and apply your polygon delineation method of choice to each of these two years separately. Compute the Jaccard Index (aka Intersection over Union) for pair(s) of selected years and present and discuss the results.

For both year one major cluster was formed for the city center of Zürich and some small clusters consisting of a rather small amount of points were formed with one more cluster for 2019 than 2018. They mostly overlap due to the major cluster and have an area of difference accounting for more than half of the total area of each year. The major clusters mostly differ in the Region of Oerlikon and Seebach as well as along the lakeside. For the year 2019 the main cluster also contains part of area of the lake, which should not be included into the cluster.



#####Mehr


```{r, echo = FALSE, message = FALSE}

#add cluster-id to each year
db18_c<- st_as_sf(as.data.frame(cbind(onlybikes18, db18$cluster)),coords = c("X", "Y"),
    crs = 2056)
db19_c<- st_as_sf(as.data.frame(cbind(onlybikes19, db19$cluster)),coords = c("X", "Y"),
    crs = 2056)
db20_c<- st_as_sf(as.data.frame(cbind(onlybikes20, db20$cluster)),coords = c("X", "Y"),
    crs = 2056)
db21_c<- st_as_sf(as.data.frame(cbind(onlybikes21, db21$cluster)),coords = c("X", "Y"),
    crs = 2056)

#comparison of plots
plot(x=db18_c$X, y=db18_c$Y, pch = 19, cex = 0.5, col = db18_c$V4 + 1, asp = 1)
title(main="2018")
plot(onlybikes18, pch = 19, cex = 0.5, col = db18$cluster + 1, asp = 1) #looks good

#split each set into clusters
  #0 is left out as it could not be clustered
db18_c_1 <- db18_c |>  filter(V4 == "1") 
db18_c_2 <- db18_c |>  filter(V4 == "2") 
db18_c_3 <- db18_c |>  filter(V4 == "3") 
db18_c_4 <- db18_c |>  filter(V4 == "4") 
db18_c_5 <- db18_c |>  filter(V4 == "5") 
db18_c_6 <- db18_c |>  filter(V4 == "6") 
db18_c_7 <- db18_c |>  filter(V4 == "7") 
db18_c_8 <- db18_c |>  filter(V4 == "8") 

db19_c_1 <- db19_c |>  filter(V4 == "1") 
db19_c_2 <- db19_c |>  filter(V4 == "2") 
db19_c_3 <- db19_c |>  filter(V4 == "3") 
db19_c_4 <- db19_c |>  filter(V4 == "4") 
db19_c_5 <- db19_c |>  filter(V4 == "5") 
db19_c_6 <- db19_c |>  filter(V4 == "6") 
db19_c_7 <- db19_c |>  filter(V4 == "7") 
db19_c_8 <- db19_c |>  filter(V4 == "8") 
db19_c_9 <- db19_c |>  filter(V4 == "9") 
db19_c_10 <- db19_c |>  filter(V4 == "10") 


#compute clusters per id
library(concaveman)
#the concavity is defined based on the biggest cluster of points
db18_c_1_c <- concaveman::concaveman(db18_c_1,concavity = 2, length_threshold = 0)
db18_c_2_c <- concaveman::concaveman(db18_c_2,concavity = 2, length_threshold = 0)
db18_c_3_c <- concaveman::concaveman(db18_c_3,concavity = 2, length_threshold = 0)
db18_c_4_c <- concaveman::concaveman(db18_c_4,concavity = 2, length_threshold = 0)
db18_c_5_c <- concaveman::concaveman(db18_c_5,concavity = 2, length_threshold = 0)
db18_c_6_c <- concaveman::concaveman(db18_c_6,concavity = 2, length_threshold = 0)
db18_c_7_c <- concaveman::concaveman(db18_c_7,concavity = 2, length_threshold = 0)
db18_c_8_c <- concaveman::concaveman(db18_c_8,concavity = 2, length_threshold = 0)

db19_c_1_c <- concaveman::concaveman(db19_c_1,concavity = 2, length_threshold = 0)
db19_c_2_c <- concaveman::concaveman(db19_c_2,concavity = 2, length_threshold = 0)
db19_c_3_c <- concaveman::concaveman(db19_c_3,concavity = 2, length_threshold = 0)
db19_c_4_c <- concaveman::concaveman(db19_c_4,concavity = 2, length_threshold = 0)
db19_c_5_c <- concaveman::concaveman(db19_c_5,concavity = 2, length_threshold = 0)
db19_c_6_c <- concaveman::concaveman(db19_c_6,concavity = 2, length_threshold = 0)
db19_c_7_c <- concaveman::concaveman(db19_c_7,concavity = 2, length_threshold = 0)
db19_c_8_c <- concaveman::concaveman(db19_c_8,concavity = 2, length_threshold = 0)
db19_c_9_c <- concaveman::concaveman(db19_c_9,concavity = 2, length_threshold = 0)
db19_c_10_c <- concaveman::concaveman(db19_c_10,concavity = 2, length_threshold = 0)



#plot all of them of 2018
ggplot() +
  geom_sf(data = boundary, color = "black", fill= NA,alpha = 0.3)+
  geom_sf(data = db18_c_1_c,  color = "red", fill = NA, alpha = 0.2) +
  geom_sf(data = db18_c_1, color = "black", size = 0.3) +
  geom_sf(data = db18_c_2_c,  color = "blue", fill = NA, alpha = 0.2) +
  geom_sf(data = db18_c_2, color = "black", size = 0.3) +
  geom_sf(data = db18_c_3_c,  color = "green", fill = NA, alpha = 0.2) +
  geom_sf(data = db18_c_3, color = "black", size = 0.3) +
  geom_sf(data = db18_c_4_c,  color = "darkgreen", fill = NA, alpha = 0.2) +
  geom_sf(data = db18_c_4, color = "black", size = 0.3) +
  geom_sf(data = db18_c_5_c,  color = "orange", fill = NA, alpha = 0.2) +
  geom_sf(data = db18_c_5, color = "black", size = 0.3) +
  geom_sf(data = db18_c_6_c,  color = "purple", fill = NA, alpha = 0.2) +
  geom_sf(data = db18_c_6, color = "black", size = 0.3) +
  geom_sf(data = db18_c_7_c,  color = "pink", fill = NA, alpha = 0.2) +
  geom_sf(data = db18_c_7, color = "black", size = 0.3) +
  geom_sf(data = db18_c_8_c,  color = "turquoise", fill = NA, alpha = 0.2) +
  geom_sf(data = db18_c_8, color = "black", size = 0.3) +
  coord_sf(datum = 2056)+
  theme_bw()

ggplot() +
  geom_sf(data = boundary, color = "black", fill= NA,alpha = 0.3)+
  geom_sf(data = db19_c_1_c,  color = "red", fill = NA, alpha = 0.2) +
  geom_sf(data = db19_c_1, color = "black", size = 0.3) +
  geom_sf(data = db19_c_2_c,  color = "blue", fill = NA, alpha = 0.2) +
  geom_sf(data = db19_c_2, color = "black", size = 0.3) +
  geom_sf(data = db19_c_3_c,  color = "green", fill = NA, alpha = 0.2) +
  geom_sf(data = db19_c_3, color = "black", size = 0.3) +
  geom_sf(data = db19_c_4_c,  color = "darkgreen", fill = NA, alpha = 0.2) +
  geom_sf(data = db19_c_4, color = "black", size = 0.3) +
  geom_sf(data = db19_c_5_c,  color = "orange", fill = NA, alpha = 0.2) +
  geom_sf(data = db19_c_5, color = "black", size = 0.3) +
  geom_sf(data = db19_c_6_c,  color = "purple", fill = NA, alpha = 0.2) +
  geom_sf(data = db19_c_6, color = "black", size = 0.3) +
  geom_sf(data = db19_c_7_c,  color = "pink", fill = NA, alpha = 0.2) +
  geom_sf(data = db19_c_7, color = "black", size = 0.3) +
  geom_sf(data = db19_c_8_c,  color = "turquoise", fill = NA, alpha = 0.2) +
  geom_sf(data = db19_c_8, color = "black", size = 0.3) +
  geom_sf(data = db19_c_9_c,  color = "lightblue", fill = NA, alpha = 0.2) +
  geom_sf(data = db19_c_9, color = "black", size = 0.3) +
  geom_sf(data = db19_c_10_c,  color = "darkorange", fill = NA, alpha = 0.2) +
  geom_sf(data = db19_c_10, color = "black", size = 0.3) +
  coord_sf(datum = 2056)+
  theme_bw()

```


```{r, echo = FALSE, message = FALSE}
#Jaquard Index etc.
all18 <- st_union(db18_c_1_c, db18_c_2_c, db18_c_3_c, db18_c_4_c, db18_c_5_c, db18_c_6_c, db18_c_7_c, db18_c_8_c)

all19 <- st_union(db19_c_1_c, db19_c_2_c, db19_c_3_c, db19_c_4_c, db19_c_5_c, db19_c_6_c, db19_c_7_c, db19_c_8_c, db19_c_9_c, db19_c_10_c, db19_c_11_c)

all18_19_dif <- st_difference(all18, all19)

plot(st_coordinates(all18), xlab = "Easting [m]", ylab = "Northing [m]", 
     type = "n", asp = 1)
plot(all18, border = "darkblue", lwd = 2, add = TRUE)
plot(all18_19_dif, col = "green", border = NA, add = TRUE)
plot(all19, col = "red", border = "red", add = TRUE)

cat("Area of 2018:", sf::st_area(all18), "m^2 \n",
    "Area of 2019: ", sf::st_area(all19), "m^2 \n",
    "Difference Area: ", sf::st_area(all18_19_dif), "m^2 \n")

```


* **Task 10**: Overall, what did you find with the above steps? What do these steps tell you about the situation of bicycle accidents in Zurich? How useful are the methods used so far in analysing the given data? Any other points of note?

I noticed, that the even the more refined methods tend to lead to a single big clusters and some small clusters. Overall clustering without including the road network, to which bicycles are bound, is not suitable and clustering using only distance to define clusters is not sufficient to discern accident- hotspots in the city centre of Zürich. 

####Mehr


# Third part of DC1 ####
* **Task 11:** Similarly to the clustering and polygon delineation tasks carried out in Parts 1 and 2 of DC1, respectively, start off by defining criteria for using KDE to detect areas/hotspots of elevated bicycle accident density, and explain your reasoning.

We need to know, that the accidents are related to each other as well as to environment (in this case informations about the road, visibility, traffic etc.) to apply KDE to it. If the accidents were not related to eachother applying KDE would be wrong. We also should think about the bandwidth we apply to the KDE and whether we want to use established rules, define one from literature or perform a iterative search, where we optimize based on validation. 


####Mehr

* **Task 12:** Choose any **two years** from the years 2018 to 2021 (justify your choice of years) and compute the KDE surfaces for each of these two separately and visualize your results. You are free to choose the KDE implementation (i.e., R package and function(s)) as well as the parameters (bandwidth selection method, etc.), but you should document your choices and discuss, in the subsequent Task 14, your results in light of your choices.

I choose the years 2018 and 2019 to be able to compare those results without having any influence from COVID and the lockdowns it caused, which affected traffic and could cause differences between the years which could make it more difficult to see how suitable a method is for assessing bicycle accident data.

###Mehr


```{r, echo = FALSE, message = FALSE}
library(adehabitatHR)

ext_val <- 0.3    # 0.3
grid_val <- 300   # 300

bikes18<- onlybikes |> 
  filter(onlybikes$AccidentYear == 2018)
bikes19<- onlybikes |> 
  filter(onlybikes$AccidentYear == 2019)

bikes18<-st_zm(bikes18[,22])
bikes19<-st_zm(bikes19[,22])

bikes18_sp <- as(bikes18, "Spatial")
bikes19_sp <- as(bikes19, "Spatial")

# First, use the reference bandwidth method for bandwith selection (h = "href")
# Positioning of legend box is optimized for knitr HTML output

# Compute UD (utilization distribution)
ud <- adehabitatHR::kernelUD(bikes18_sp, grid = grid_val, extent = ext_val, h = "href")
hr95 <- adehabitatHR::getverticeshr(ud, percent = 95)   # retrieve home range (95th volume percentile)
hr50 <- adehabitatHR::getverticeshr(ud, percent = 50)   # retrieve core area (50th volume percentile)

graphics::image(ud, xlab = "x [m]", ylab = "y [m]",
                col = hcl.colors(200, palette = "heat 2", rev = TRUE))
xmin <- min(ud$ZH@coords[,1])
xmax <- max(ud$ZH@coords[,1])
ymin <- min(ud$ZH@coords[,2])
ymax <- max(ud$ZH@coords[,2])
plot(hr50, lty = 4, lwd = 3, border = "black", add = TRUE, axes = FALSE)
plot(hr95, lty = 1, lwd = 2, border = "blue", add = TRUE, axes = FALSE)
axis(1)
axis(2, pos = xmin - 100)
text(xmin - 150, ymin + (ymax - ymin) / 2, "y [m]", 
     adj = c(NA, -4), srt = 90)
title("KDE with bandwidth selection method HREF", line = -0.3)
legend("topright", c("HR 50%", "HR 95%"), 
       col = c("black", "blue"), lwd = c(3, 2), lty = c(4, 1), 
       inset = c(0.19, 0.06), cex = 0.75)

##### plug in hpi-----
library(ks)

onlybikes18 <-onlybikes18[,-3]
onlybikes19 <-onlybikes19[,-3]

h18 <- ks::Hpi(x = onlybikes18)
h19 <- ks::Hpi(x = onlybikes19)

fkde18 <- ks::kde(onlybikes18, H = h18)
fkde19 <- ks::kde(onlybikes19, H = h19)

plot(fkde18, display = "filled.contour2", 
     cont = c(10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 100),  # percentage contours
     main = "Unweighted KDE; plug-in bandwidth", asp = 1)
plot(bikes18, cex = 0.5, pch = 16, col = "black", add = TRUE)


plot(fkde19, display = "filled.contour2", 
     cont = c(10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 100),  # percentage contours
     main = "Unweighted KDE; plug-in bandwidth", asp = 1)

plot(bikes19, cex = 0.5, pch = 16, col = "black", add = TRUE)


adehabitatHR::

```



* **Task 13:** Compute the "volume of intersection" ("VI") between the KDE surfaces (utilization distributions) of the two years. Hint: There are different ways to do that, but the `adehabitatHR` package has functionality for that. How do the results correspond to those of Task 9 (Jaccard Index or IoU)?

```{r}

a<- st_as_sf(hr50)
b<- st_as_sf(hr95)
plot(a)
plot(b)
ggplot() +
  geom_sf(data = a, color = "black", fill= NA,alpha = 0.3)+
  geom_sf(data = b, color = "black", fill= NA,alpha = 0.3)

#extrahieren bei adehabitat geht

```


* **Task 14:** Discuss your results for this part of DC1 (density estimation). What did you find? Compare the results of this part with the clusters/polygons of Parts 1 and 2 (see note below): What are the commonalities? What are the differences? Which method(s) perform more adequately than others for the given problem and data? Which method(s) would you recommend, and which ones not? Why? (You are free to add more points to the discussion.)

# Fourth part of DC1 ####
übernommen von Saskia

* **Task 15:** Choose one or more distance measure functions (justify your choice) and compute
it/them for * a. all bicycle accidents (2011 - 2021) 
I chose the L-function as it's based on the K-function but linear and thus easier to intrepret if there is correlation.

::: {.panel-tabset .nav-pills}

## L-Function

```{r, echo=FALSE}
# Create a ppp object 
bike_accidents_coords <- sf::st_coordinates(onlybikes)
win <- as.owin(st_bbox(onlybikes))
bike_accidents_ppp <- spatstat.geom::ppp(x = bike_accidents_coords[,1], y =  bike_accidents_coords[,2],
                                         window = win, unitname = c("meter", "meters"))

# Generate L-function with envelope() function.
bike_accidents_enve <- spatstat.explore::envelope(bike_accidents_ppp, fun = Lest, nsim = 100, correction = "Ripley", verbose = FALSE)
# Plot 
plot(bike_accidents_enve, main = "L-function of bike acccident points, with envelope")

```
## MAD & DCLF Test
```{r, echo=FALSE}
# We also apply the MAD test
spatstat.explore::mad.test(bike_accidents_ppp, fun = Lest, nsim = 100, verbose = FALSE)
spatstat.explore::dclf.test(bike_accidents_ppp, fun = Lest, nsim = 100, verbose = TRUE)

```
:::

* b. bicycle accidents for one of the years selected in Task 6 

::: {.panel-tabset .nav-pills}
## L-Function 2018 data
```{r, echo=FALSE}
# Create a ppp object 

bike_accidents2018_coords <- sf::st_coordinates(filter(onlybikes, AccidentYear ==2018))
win18 <- as.owin(st_bbox(filter(onlybikes, AccidentYear ==2018)))
bike_accidents2018_ppp <- spatstat.geom::ppp(x = bike_accidents2018_coords[,1], y =  bike_accidents2018_coords[,2],
                                         window = win18, unitname = c("meter", "meters"))

# Generate L-function
bike_accidents2018_enve <- spatstat.explore::envelope(bike_accidents2018_ppp, fun = Lest, nsim = 100, correction = "Ripley", verbose = FALSE)
# Plot 
plot(bike_accidents2018_enve, main = "L-function of 2018 bike acccident points, with envelope")

```

## MAD & DCLF Test 2018 data
```{r, echo=FALSE}
# We also apply the MAD test
spatstat.explore::mad.test(bike_accidents2018_ppp, fun = Lest, nsim = 100, verbose = FALSE)
spatstat.explore::dclf.test(bike_accidents2018_ppp, fun = Lest, nsim = 100, verbose = TRUE)

```

## L-Function 2019 data
```{r, echo=FALSE}
# Create a ppp object 
bike_accidents2019_coords <- sf::st_coordinates(filter(onlybikes, AccidentYear ==2019))
win19 <- as.owin(st_bbox(filter(onlybikes, AccidentYear ==2019)))
bike_accidents2019_ppp <- spatstat.geom::ppp(x = bike_accidents2019_coords[,1], y =  bike_accidents2019_coords[,2],
                                         window = win19, unitname = c("meter", "meters"))

# Generate L-function
bike_accidents2019_enve <- spatstat.explore::envelope(bike_accidents2019_ppp, fun = Lest, nsim = 100, 
                                         correction = "Ripley", verbose = FALSE)
# Plot 
plot(bike_accidents2019_enve, main = "L-function of 2019 bike acccident points, with envelope")
```

## MAD & DCLF Test 2019 data
```{r, echo=FALSE}
# We also apply the MAD test
spatstat.explore::mad.test(bike_accidents2019_ppp, fun = Lest, nsim = 100, verbose = FALSE)
spatstat.explore::dclf.test(bike_accidents2019_ppp, fun = Lest, nsim = 100, verbose = TRUE)
```

::: 


* c. bicycle accidents for only the cluster points of the year selected in (b)
Restrict your analysis to the “inner city” and use the same window for all point sets. It’s up to you to define the extent of the “inner city” and explain/justify what that means in terms of this data challenge.

##intersection
```{r}
districts145<-districts[10:12,]

dist145_2018<-st_intersection(filter(onlybikes, AccidentYear ==2018),districts145)

ggplot()+
  geom_sf(data=boundary, aes(color="white"))+
  geom_sf(data=dist145_2018, aes(color="black"))+
  theme(legend.position="none")#looks good

```


::: {.panel-tabset .nav-pills}
## Choosing a subset of districts
```{r, echo=FALSE}
ggplot(districts)+
  geom_sf(aes(fill=KNR))+
  geom_sf_label(aes(label = KNAME))+
  theme_minimal() +
  coord_sf(datum = NA)+
  xlab("")+
  ylab("")
```

## L-Function 2018 data
```{r, echo=FALSE}
# Create a ppp object 

bike_accidents2018_coords145 <- sf::st_coordinates(dist145_2018)
win18_145 <- as.owin(st_bbox(dist145_2018))
bike_accidents2018_ppp_145 <- spatstat.geom::ppp(x = bike_accidents2018_coords145[,1], y =  bike_accidents2018_coords145[,2],
                        window = win18, unitname = c("meter", "meters"))

# Generate L-function
bike_accidents2018_enve145 <- spatstat.explore::envelope(bike_accidents2018_ppp_145, fun = Lest, nsim = 100, correction = "Ripley", verbose = FALSE)
# Plot 
plot(bike_accidents2018_enve145, main = "L-function of 2018 bike acccident points, with envelope for the districts 1,4 and 5")

```

## MAD & DCLF Test 2018 data
```{r, echo=FALSE}
# We also apply the MAD test
spatstat.explore::mad.test(bike_accidents2018_ppp_145, fun = Lest, nsim = 100, verbose = FALSE)
spatstat.explore::dclf.test(bike_accidents2018_ppp_145, fun = Lest, nsim = 100, verbose = TRUE)

```
:::

* **Task 16:** Now choose the following two pairs of years, 2018 & 2019, as well as 2018 & 2021, and compute the cross-X function, where “X” stands for the function(s) you used in Task 15. Use the AccidentYear as the marks to produce a marked point pattern.


::: {.panel-tabset .nav-pills}

## marked point pattern
```{r, echo=FALSE}
# reate a marked ppp object
marks_bike_accidents_ppp <- spatstat.geom::setmarks(bike_accidents_ppp, factor(onlybikes$AccidentYear))

bike_accidents181921 <- onlybikes|> 
  filter(onlybikes$AccidentYear %in% c("2018", "2019", "2021"))

# Summary will show frequency table per Year
summary(bike_accidents181921)

ggplot() +
  geom_sf(data = bike_accidents181921, aes(colour = AccidentYear))  +
  coord_sf(datum = 2056) +
  ggtitle("Marked point pattern of bike accidents 2018, 2019 and 2021") +
  xlab("Easting [m]") + 
  ylab("Northing [m]") +
  theme(plot.title = element_text(face="bold"))
```

## Cross-L function 2018 and 2019
```{r, echo=FALSE}
acc_2018_2019_clf_env <- spatstat.explore::envelope(marks_bike_accidents_ppp, fun = Lcross, 
                                                    i = "2018", j = "2019", nsim = 100, 
                                                    correction = "Ripley", verbose = FALSE)

# Plot it
plot(acc_2018_2019_clf_env, main = "Cross-L-function of bike accidents 2018 vs. 2019")
```

## Cross-L function 2018 and 2021
```{r, echo=FALSE}
#create ppp for 2021 data
bike_accidents_2021_sf <- onlybikes |> 
  filter(onlybikes$AccidentYear == 2021)

# drop everything except geometry
bike_accidents_2021_sf <- st_zm(bike_accidents_2021_sf[,22])

bike_accidents2021_coords <- sf::st_coordinates(bike_accidents_2021_sf)
win21 <- as.owin(st_bbox(bike_accidents_2021_sf))
bike_accidents2021_ppp <- spatstat.geom::ppp(x = bike_accidents2021_coords[,1], y =  bike_accidents2021_coords[,2],
                                         window = win21, unitname = c("meter", "meters"))


acc_2018_2021_clf_env <- spatstat.explore::envelope(marks_bike_accidents_ppp, fun = Lcross, 
                                                    i = "2018", j = "2021", nsim = 100, 
                                                    correction = "Ripley", verbose = FALSE)

# Plot it
plot(acc_2018_2021_clf_env, main = "Cross-L-function of bike accidents 2018 vs. 2021")
```
:::

* **Task 17:** Discuss your results. Similarities and differences between the various point sets? Noteworthy spatial and/or temporal patterns? Is any difference observable in the patterns of the cross-X function in the transition to the Covid-19 pandemic? etc. etc. Note: Consider in your interpretation that the distance scale may change between years.

KOMMENTAR


* **Task 18:** From the bicycle accidents data, choose at least two relevant variables that make sense being compared, e.g. two different accident types, severity levels, times of day, years etc. For these selected accidents, compute the counts within the ‘statistical zones’ of Zurich and use these counts to compute the Getis-Ord G*-statistic for each of your counts layers. Visualize your results appropriately.

```{r}
# read data of statistical zones
zh_stat_zone <- sf::st_read("Session_5/zh_stat_zones.gpkg")

# Find nearest neighbors within 500m
# You should play with nn.dis and see what it does to connectivity.
nn_dis <- 500

# create list of euclidean neighbors with min. distance = 0 and max. distance = nn_dis 
bike_acc_nb <- spdep::dnearneigh(filter(onlybikes, AccidentSeverityCategory_en == c("Accident with severe injuries"
,"Accident with light injuries") ), 0, nn_dis)

# convert into list with spatial weights, B = binary weight
bike_acc_lw <- spdep::nb2listw(spdep::include.self(bike_acc_nb), style = "B")

# Create the line links from the nb object, requesting an sf object to be returned.
# CRS needs to be set as it was carried over to the nb object.
bike_acc_links <- 
   spdep::nb2lines(bike_acc_nb, coords = sf::st_geometry(filter(onlybikes, AccidentSeverityCategory_en == c("Accident with severe injuries"
,"Accident with light injuries"))), as_sf = TRUE) |>
   sf::st_set_crs(2056)
# 
# # Plot the neighbor links using functions from the tmap package
tmap::tm_shape(districts) + 
  tmap::tm_borders(col = "gray") +
  tmap::tm_shape(bike_acc_links) + 
  tmap::tm_lines(col = "red", lwd = 0.5) +
  tmap::tm_shape(filter(onlybikes, AccidentSeverityCategory_en == c("Accident with severe injuries"
,"Accident with light injuries"))) +
  tmap::tm_dots(col = "black", size = 0.1) +
  tmap::tm_scale_bar(position = c("right", "bottom"), width = 0.2)
   
```

```{r}
#| label: g-stat-compute
# Initialize variable names for variants in sads_comp 
# --> we use the proportions per variant
var_string <- c("prop_v1_als", "prop_v2_weder", "prop_v3_wie", "prop_v4_wan")
variant <- c("als", "weder", "wie", "wan")

# For each variant, compute local Gi*-statistic with spdep::localG() and 
# display as choropleth map.

# Define a function that, for a given variant, computes the local Gi*-statistic 
# with spdep::localG() and displays it as a choropleth map.
plot_variant <- function(i, j) {

    # Create local Gi* stats object
  sads_lG <-
    spdep::localG(x = as.numeric(unlist(sads_comp_df[, i])), listw = sads_lw)
  
  # attach Gi* statistic values to the sfc_POINT object
  s_lG <- sads_lG[1:length(sads_lG)]   # convert to vector
  tmp_sf <- bind_cols(sads_comp_sf, as.data.frame(s_lG))
  
  # generate map with dot symbols shaded according to Gi*-stats value
  m <- tmap::tm_shape(cantons_bound) +
    tmap::tm_borders(col = "gray") +
    tmap::tm_shape(tmp_sf) +
    tmap::tm_dots(
      size = 0.2,
      col = "s_lG",
      n = 6,
      midpoint = 0,
      palette = "-RdYlBu",
      title.size = 0.2,
      title = paste("Variant \"", variant[j], "\"", sep = "")
    ) +
    tmap::tm_layout(
      main.title = "Gi*-statistic for comparative clause",
      main.title.size = 1.0,
      main.title.fontface = "bold",
      main.title.position = c("left", "top"),
      legend.position = c("left", "top"),
      frame = FALSE,
      inner.margins = c(0.05, 0.05, 0.05, 0.05)
    ) +
    tmap::tm_scale_bar(position = c("right", "bottom"), width = 0.2)
  
  # a tmap inside a loop needs to be *printed* explicitly; otherwise it won't show
  # print(m)
  
  return(m)
}

```


* **Task 19:** Discuss your results. What did you find regarding the hot and cold spots in your accident count layers? How do they compare to each other across layers and across (past) methods? etc. Discuss also the influence of the parameter settings (e.g., neighbor search distance, spatial weight formation) on your results.

KOMMENTAR

# Fifth part of DC1 ####


